{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Creating DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "myspark=SparkSession.builder.appName(\"Spark_DF_Operations\").master(\"yarn\")\\\n",
    "    .config(\"spark.executor.memory\",\"4g\")\\\n",
    "    .enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Using List of Tuples [(\"Naresh\",25)] or List of Lists [[\"Naresh\",25]] or List of Dict [{\"name\":\"naresh\", \"age\":25}]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using List of Tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Type of Record is :  <type 'tuple'> \n",
      "\n",
      "+------+---+\n",
      "|    _1| _2|\n",
      "+------+---+\n",
      "|Naresh| 25|\n",
      "|  Ravi| 22|\n",
      "| Bhanu| 20|\n",
      "| Akash| 26|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "l = [('Naresh',25),('Ravi',22),('Bhanu',20),('Akash',26)]\n",
    "print \"\\nType of Record is : \", type(('Naresh',25)) , \"\\n\"\n",
    "df = myspark.createDataFrame(l)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using List of Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Type of Record is :  <type 'list'> \n",
      "\n",
      "+------+------+---+-----+\n",
      "|    _1|    _2| _3|   _4|\n",
      "+------+------+---+-----+\n",
      "|naresh|jangra| 33|170.5|\n",
      "|  ravi| verma| 30|150.5|\n",
      "+------+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "l = [[\"naresh\",\"jangra\",33,170.5],[\"ravi\",\"verma\",30,150.5]]\n",
    "print \"\\nType of Record is : \", type([\"naresh\",\"jangra\",33,170.5]) , \"\\n\"\n",
    "myspark.createDataFrame(l).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using List of Dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# l = [{\"fname\":\"naresh\", \"lname\":\"jangra\", \"age\": 33, \"height\": 170.5},{\"fname\":\"ravi\", \"lname\":\"verma\", \"age\": 53, \"height\": 150.6}]\n",
    "# print \"\\nType of Record is : \", type({\"fname\":\"naresh\", \"lname\":\"jangra\", \"age\": 33, \"height\": 170.5}) , \"\\n\"\n",
    "# myspark.createDataFrame(l).show()\n",
    "\n",
    "#1. Column must have the same type of data. For eq. for first record height is DoubleType. It should be Double for all records.\n",
    "# Having int height (150) will cause DF creation failure saying: \n",
    "# TypeError: Can not merge type <class 'pyspark.sql.types.DoubleType'> and <class 'pyspark.sql.types.LongType'>\n",
    "\n",
    "#2. /opt/cloudera/parcels/SPARK2-2.1.0.cloudera1-1.cdh5.7.0.p0.120904/lib/spark2/python/pyspark/sql/session.py:320: \n",
    "# UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
    "#  warnings.warn(\"inferring schema from dict is deprecated,\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error if we have the string (\"Naresh\") instead of Tuple (\"Naresh\",25)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "l = [('Naresh'),('Ravi'),('Bhanu'),('Akash')]\n",
    "type('Naresh')\n",
    "\n",
    "df = myspark.createDataFrame(l)\n",
    "\n",
    "TypeError: Can not infer schema for type: <type 'str'>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using List of pyspark.sql.Row() Objects\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A row in DataFrame. The fields in it can be accessed: like attributes (row.key) , like dictionary values (row[key])\n",
    "Row can be used to create a row object by using named arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Name is :  Naresh\n",
      "\n",
      "Age is :  34\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "row = Row(name=\"Naresh\", age=34)\n",
    "print \"\\nName is : \" , row.name\n",
    "print \"\\nAge is : \" ,row[\"age\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Row also can be used to create another Row like class, then it could be used to create Row objects, such as\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(name='Naresh', age=34)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Person = Row(\"name\", \"age\")\n",
    "Person(\"Naresh\", 34)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Dataframe from List of Row()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|age|  name|\n",
      "+---+------+\n",
      "| 33|naresh|\n",
      "| 44|  ravi|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rowlist = [Row(name = \"naresh\", age = 33),Row(name = \"ravi\", age = 44)]\n",
    "df = myspark.createDataFrame(rowlist)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Using Row and list separately\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naresh 22\n"
     ]
    }
   ],
   "source": [
    "# How to Access Tupe\n",
    "tup = (\"naresh\",22)\n",
    "print tup[0], tup[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|age|  name|\n",
      "+---+------+\n",
      "| 25|Naresh|\n",
      "| 22|  Ravi|\n",
      "| 20| Bhanu|\n",
      "| 26| Akash|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "l = [('Naresh',25),('Ravi',22),('Bhanu',20),('Akash',26)]\n",
    "rdd = myspark.sparkContext.parallelize(l)\n",
    "data = rdd.map(lambda x: Row(name=x[0], age=x[1]))\n",
    "df = myspark.createDataFrame(data)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Using data files of Different Formats\n",
    "\n",
    "The data files used below is available in this repo under data folder. LOAD this file in the HDFS at /tmp directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formats: csv, tsv, psv\n",
    "\n",
    "Below is an example for csv data file. For tsv(Table separated) and psv(pipe separated) data just use option sep=\" \"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+---+------+---------------------+---------------------+\n",
      "|fname  |lname  |age|height|dated                |timing               |\n",
      "+-------+-------+---+------+---------------------+---------------------+\n",
      "|naresh |jangra |30 |170.5 |2013-10-12 00:00:00.0|2013-10-12 12:35:50.0|\n",
      "|ravi   |verma  |35 |155.67|2014-10-12 00:00:00.0|2014-10-12 01:55:50.0|\n",
      "|viren  | nain  |55 |160.0 |2015-10-12 00:00:00.0|2015-10-12 09:15:50.0|\n",
      "|bhanu  | pratap|11 |180.8 |2016-10-12 00:00:00.0|2016-10-12 10:05:50.0|\n",
      "+-------+-------+---+------+---------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = myspark.read.csv(\"/tmp/sampledata.csv\", \\\n",
    "                header=True, inferSchema=True, dateFormat=\"yyyy-MM-dd\", timestampFormat=\"yyyy-MM-dd HH:mm:ss\")\n",
    "\n",
    "df.show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+---+------+---------------------+---------------------+\n",
      "|fname  |lname  |age|height|dated                |timing               |\n",
      "+-------+-------+---+------+---------------------+---------------------+\n",
      "|naresh |jangra |30 |170.5 |2013-10-12 00:00:00.0|2013-10-12 12:35:50.0|\n",
      "|ravi   |verma  |35 |155.67|2014-10-12 00:00:00.0|2014-10-12 01:55:50.0|\n",
      "|viren  | nain  |55 |160.0 |2015-10-12 00:00:00.0|2015-10-12 09:15:50.0|\n",
      "|bhanu  | pratap|11 |180.8 |2016-10-12 00:00:00.0|2016-10-12 10:05:50.0|\n",
      "+-------+-------+---+------+---------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = myspark.read.format(\"csv\").option(\"header\",True).option(\"inferSchema\",True)\\\n",
    "    .option(\"dateFormat\",\"yyyy-MM-dd\").option(\"timestampFormat\",\"yyyy-MM-dd HH:mm:ss\")\\\n",
    "    .option(\"path\",\"/tmp/sampledata.csv\").load()\n",
    "    \n",
    "df.show(10,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+---+------+---------------------+---------------------+\n",
      "|fname |lname |age|height|dated                |timing               |\n",
      "+------+------+---+------+---------------------+---------------------+\n",
      "|naresh|jangra|30 |170.5 |2013-10-12 00:00:00.0|2013-10-12 12:35:50.0|\n",
      "|ravi  |verma |35 |155.67|2014-10-12 00:00:00.0|2014-10-12 01:55:50.0|\n",
      "|viren |nain  |55 |160.0 |2015-10-12 00:00:00.0|2015-10-12 09:15:50.0|\n",
      "|bhanu |pratap|11 |180.8 |2016-10-12 00:00:00.0|2016-10-12 10:05:50.0|\n",
      "+------+------+---+------+---------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = myspark.read.format(\"csv\").options(header=True, inferSchema=True, sep=\",\",\n",
    "    dateFormat=\"yyyy-MM-dd\", timestampFormat=\"yyyy-MM-dd HH:mm:ss\", ignoreLeadingWhiteSpace=True,\\\n",
    "    ignoreTrailingWhiteSpace=True, path=\"/tmp/sampledata.csv\").load()\n",
    "\n",
    "df.show(10,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schema inferred using \"inferSchema=True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- fname: string (nullable = true)\n",
      " |-- lname: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      " |-- dated: timestamp (nullable = true)\n",
      " |-- timing: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Formats: text\n",
    "    \n",
    "    You can simply use spark.read.csv() and just set the sep \" \"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+---+------+----------+----------+\n",
      "|  name|gender|age|height|     dated|    timing|\n",
      "+------+------+---+------+----------+----------+\n",
      "|  ravi| verma| 35|155.67|2014-10-12|2014-10-12|\n",
      "| bhanu|pratap| 11| 180.8|2016-10-12|2016-10-12|\n",
      "| viren|  nain| 55| 160.0|2015-10-12|2015-10-12|\n",
      "|naresh|jangra| 30| 170.5|2013-10-12|2013-10-12|\n",
      "+------+------+---+------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Removing the Header First\n",
    "file = myspark.sparkContext.textFile(\"/tmp/sampledata.txt\")\n",
    "fileheader = file.first()\n",
    "header = myspark.sparkContext.parallelize([fileheader])\n",
    "rdd = file.subtract(header)\n",
    "\n",
    "# Loading the data in required format. You can always change cast the Datatype later\n",
    "line = rdd.map(lambda x: x.split(\" \")).map(lambda t: (t[0],t[1],int(t[2]),float(t[3]),t[4],t[5]))\n",
    "df = myspark.createDataFrame(line,[\"name\",\"gender\",\"age\",\"height\",\"dated\",\"timing\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format: ORC and Parquet\n",
    "\n",
    "Saving the previous df as parquet and orc formats in HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.write.save(path = \"/tmp/sampledata_parq\", format=\"parquet\", mode=\"overwrite\")\n",
    "df.write.save(path = \"/tmp/sampledata_orc\", format=\"orc\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the Parquet Data as DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+---+------+----------+----------+\n",
      "|name  |gender|age|height|dated     |timing    |\n",
      "+------+------+---+------+----------+----------+\n",
      "|viren |nain  |55 |160.0 |2015-10-12|2015-10-12|\n",
      "|naresh|jangra|30 |170.5 |2013-10-12|2013-10-12|\n",
      "|bhanu |pratap|11 |180.8 |2016-10-12|2016-10-12|\n",
      "|ravi  |verma |35 |155.67|2014-10-12|2014-10-12|\n",
      "+------+------+---+------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = myspark.read.parquet(\"/tmp/sampledata_parq\")\n",
    "df.show(10,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schema information will be taken from Parquet. You can cast to required Datatype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      " |-- dated: string (nullable = true)\n",
      " |-- timing: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the Orc Data as DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+---+------+----------+----------+\n",
      "|name  |gender|age|height|dated     |timing    |\n",
      "+------+------+---+------+----------+----------+\n",
      "|viren |nain  |55 |160.0 |2015-10-12|2015-10-12|\n",
      "|naresh|jangra|30 |170.5 |2013-10-12|2013-10-12|\n",
      "|bhanu |pratap|11 |180.8 |2016-10-12|2016-10-12|\n",
      "|ravi  |verma |35 |155.67|2014-10-12|2014-10-12|\n",
      "+------+------+---+------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = myspark.read.orc(\"/tmp/sampledata_orc\")\n",
    "df.show(10,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schema information will be taken from Orc. You can cast to required Datatype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      " |-- dated: string (nullable = true)\n",
      " |-- timing: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format: json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------+------+------+-------------------+\n",
      "|age|     dated| fname|height| lname|             timing|\n",
      "+---+----------+------+------+------+-------------------+\n",
      "| 30|2013-10-12|naresh| 170.5|jangra|2013-10-12 12:35:50|\n",
      "| 35|2014-10-12|  ravi|155.67| verma|2014-10-12 01:55:50|\n",
      "| 55|2015-10-12| viren| 160.0|  nain|2015-10-12 09:15:50|\n",
      "| 11|2016-10-12| bhanu| 180.8|pratap|2016-10-12 10:05:50|\n",
      "+---+----------+------+------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = myspark.read.json(\"/tmp/sampledata.json\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or just like csv data reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------+------+------+-------------------+\n",
      "|age|dated     |fname |height|lname |timing             |\n",
      "+---+----------+------+------+------+-------------------+\n",
      "|30 |2013-10-12|naresh|170.5 |jangra|2013-10-12 12:35:50|\n",
      "|35 |2014-10-12|ravi  |155.67|verma |2014-10-12 01:55:50|\n",
      "|55 |2015-10-12|viren |160.0 |nain  |2015-10-12 09:15:50|\n",
      "|11 |2016-10-12|bhanu |180.8 |pratap|2016-10-12 10:05:50|\n",
      "+---+----------+------+------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = myspark.read.format(\"json\").options(header=True, inferSchema=True,\n",
    "    dateFormat=\"yyyy-MM-dd\", timestampFormat=\"yyyy-MM-dd HH:mm:ss\", ignoreLeadingWhiteSpace=True,\\\n",
    "    ignoreTrailingWhiteSpace=True, path=\"/tmp/sampledata.json\").load()\n",
    "\n",
    "df.show(10,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schema information will be inferred(as much possible). You can cast to required Datatype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- dated: string (nullable = true)\n",
      " |-- fname: string (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      " |-- lname: string (nullable = true)\n",
      " |-- timing: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Using Hive Tables\n",
    "    \n",
    "Simply read any existing table using myspark.sql"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df = myspark.sql(\"SELECT * FROM default.mytest where fname='naresh'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Applying SCHEMA on the Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we use pyspark.sql.Row(), or read data from any data file source, we can always get/infer the Schema(). \n",
    "\n",
    "Sometime, we may need to apply Schema separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Schema by passing a List of Col Names.\n",
    "\n",
    "When schema is a list of column names, the type of each column will be inferred from data.\n",
    "\n",
    "Let us apply this on the same df we generated in the very first step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data = [[\"naresh\",\"jangra\",33,170.5],[\"Ravi\",\"verma\",20,150.5]]\n",
    "#or\n",
    "data = [(\"naresh\",\"jangra\",33,170.5),(\"Ravi\",\"verma\",20,150.5)]\n",
    "type((\"naresh\",\"jangra\",33,170.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+---+------+\n",
      "| fname| lname|age|height|\n",
      "+------+------+---+------+\n",
      "|naresh|jangra| 33| 170.5|\n",
      "|  Ravi| verma| 20| 150.5|\n",
      "+------+------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = myspark.createDataFrame(data,[\"fname\",\"lname\",\"age\",\"height\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- fname: string (nullable = true)\n",
      " |-- lname: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Passing Scehma using pyspark.sql.types.StructType()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = [[\"naresh\",\"jangra\",33,170.5],[\"Ravi\",\"verma\",20,150.5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from pyspark.sql import types as T\n",
    "schema = T.StructType([\n",
    "                T.StructField(\"fname\", T.StringType(), True),\n",
    "                T.StructField(\"lname\", T.StringType(), True),\n",
    "                T.StructField(\"age\", T.IntegerType(), True),\n",
    "                T.StructField(\"height\", T.DoubleType(), True)\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+---+------+\n",
      "| fname| lname|age|height|\n",
      "+------+------+---+------+\n",
      "|naresh|jangra| 33| 170.5|\n",
      "|  Ravi| verma| 20| 150.5|\n",
      "+------+------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = myspark.createDataFrame(data,schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- fname: string (nullable = true)\n",
      " |-- lname: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Inferring Scehma from Data File (with Seperators like \",\" , \"|\", \"\\t\", \" \" etc)\n",
    "\n",
    "    We have already done this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+---+------+---------------------+---------------------+\n",
      "|fname |lname |age|height|dated                |timing               |\n",
      "+------+------+---+------+---------------------+---------------------+\n",
      "|naresh|jangra|30 |170.5 |2013-10-12 00:00:00.0|2013-10-12 00:00:00.0|\n",
      "|ravi  |verma |35 |155.67|2014-10-12 00:00:00.0|2014-10-12 00:00:00.0|\n",
      "|viren |nain  |55 |160.0 |2015-10-12 00:00:00.0|2015-10-12 00:00:00.0|\n",
      "|bhanu |pratap|11 |180.8 |2016-10-12 00:00:00.0|2016-10-12 00:00:00.0|\n",
      "+------+------+---+------+---------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = myspark.read.format(\"csv\").options(header=True, inferSchema=True, sep=\" \",\n",
    "    dateFormat=\"yyyy-MM-dd\", timestampFormat=\"yyyy-MM-dd HH:mm:ss\", ignoreLeadingWhiteSpace=True,\\\n",
    "    ignoreTrailingWhiteSpace=True, path=\"/tmp/sampledata.txt\").load()\n",
    "\n",
    "df.show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- fname: string (nullable = true)\n",
      " |-- lname: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      " |-- dated: timestamp (nullable = true)\n",
      " |-- timing: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can always do the formating (Changing format of timing Column) and casting (from timestamp to DateType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df.withColumn(\"dated\", F.to_date(\"dated\").cast(T.DateType()))\\\n",
    " .withColumn(\"timing\", F.from_unixtime(F.unix_timestamp(\"timing\"),\"yyyy-MM-dd HH:mm:ss\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+---+------+----------+-------------------+\n",
      "|fname |lname |age|height|dated     |timing             |\n",
      "+------+------+---+------+----------+-------------------+\n",
      "|naresh|jangra|30 |170.5 |2013-10-12|2013-10-12 00:00:00|\n",
      "|ravi  |verma |35 |155.67|2014-10-12|2014-10-12 00:00:00|\n",
      "|viren |nain  |55 |160.0 |2015-10-12|2015-10-12 00:00:00|\n",
      "|bhanu |pratap|11 |180.8 |2016-10-12|2016-10-12 00:00:00|\n",
      "+------+------+---+------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's Next\n",
    "\n",
    "1) To Download this Single Notebook, go to Click this file in my Github Account, Copy the URL and paste in http://nbviewer.jupyter.org/. Download button will be in top right corner.\n",
    "\n",
    "2) Open your Juypter Notebook home page and upload using \"upload\" Button.\n",
    "\n",
    "3) Continue Learning from the next Notebook Spark_03_Saving_DataFrames.ipynb"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
