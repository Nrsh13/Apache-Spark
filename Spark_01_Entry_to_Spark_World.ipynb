{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entering Sprak World"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Using SparkContext (Spark Version < 2.x)\n",
    "In Spark versions < 2.x, we used to create a SparkConf and SparkContext to interact with Spark as we will see below. We can do the same in spark2 as well but SparkSession(not SparkContext) will be the main entry point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating sconf variable using SparkConf (which will be used for SparkContext Creation) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sconf = SparkConf().setAppName(\"SparkContext_Usage\").set(\"spark.executor.memory\",\"6g\").set(\"spark.executor.cores\",\"3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating SparkContext 'sc'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SparkContext: Main entry point for Spark functionality. A SparkContext represents the connection to a Spark cluster, and can be used to create RDDs, accumulators and broadcast variables on that cluste\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, if we stop sc (sc.stop()) to create a SparkContext with new name in pyspark shell, it will automatically come up again. So, DONOT Create any new SparkContext \"sc\". sc will get Killed with ERROR \"SparkContext has been shutdown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "sc = SparkContext(conf=sconf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Configuration Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'6g'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sc.getConf(\"spark.executor.memory\")\n",
    "sconf.get(\"spark.executor.memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'4g'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sconf.set(\"spark.executor.memory\",\"4g\")\n",
    "sconf.get(\"spark.executor.memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQLContext: To Create DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating SQLContext 'sqlCtx'\n",
    "\n",
    "The entry point into all 'RELATIONAL FUNCTIIONALITY' in Spark is the SQLContext class. Enables working with structured data (rows and columns) in Spark. Allows the creation of DataFrame objects as well as the execution of SQL queries. \n",
    "\n",
    "When created, SQLContext adds a method called toDF to RDD, which could be used to convert an RDD into a DataFrame, it is a shorthand for SQLContext.createDataFrame().\n",
    "\n",
    "Make sure you use toDF() only after you create the SQLContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|    _1| _2|\n",
      "+------+---+\n",
      "|Naresh|  1|\n",
      "| Bhanu|  2|\n",
      "|  Ravi|  3|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SQLContext, sql\n",
    "\n",
    "sqlCtx = SQLContext(sc)\n",
    "rdd = sc.parallelize([('Naresh',1),('Bhanu',2),('Ravi',3)])\n",
    "rdd.toDF().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data from HDFS and Create DF using sqlCtx\n",
    "\n",
    "The data file used below is available in this repo under data folder. LOAD this file in the HDFS at /tmp directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+---+--------+\n",
      "|   name|gender|age|language|\n",
      "+-------+------+---+--------+\n",
      "| naresh|     M| 30|  python|\n",
      "|   ravi|     M| 22|       c|\n",
      "|akansha|     F| 34|    java|\n",
      "| ravina|     F| 33|    ruby|\n",
      "|  bhanu|     M| 35|    java|\n",
      "|  vikas|     M| 11|   shell|\n",
      "|   jiya|     F| 39|    perl|\n",
      "+-------+------+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file = sc.textFile(\"/tmp/people.txt\")\n",
    "line = file.map(lambda x: x.split(\" \")).map(lambda t: (t[0],t[1],int(t[2]),t[3]))\n",
    "df = sqlCtx.createDataFrame(line,[\"name\",\"gender\",\"age\",\"language\"])\n",
    "df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HiveContext: To access Hive DB/Tables or save data to Hive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating HiveContext 'hqlCtx'\n",
    "\n",
    "DataFrames can also be saved as persistent tables into Hive metastore using the saveAsTable command. If you have configured your existing HIVE setup (by putting hive-site.xml in Spark conf directory), you can check the new table there. But if you donot have Hive Metastore setup, Spark will create a default local Hive metastore (using Derby) for you. \n",
    "\n",
    "Unlike the createOrReplaceTempView command, saveAsTable will materialize the contents of the DataFrame and create a pointer to the data in the Hive metastore.By default saveAsTable will create a 'managed table', meaning that the location of the data will be controlled by the metastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dropping Hive Table if exists\n",
      "\n",
      "Save Data Frame to Hive table default.nartest\n",
      "\n",
      "Query Newly created Hive Table \n",
      "+-------+------+---+--------+\n",
      "|   name|gender|age|language|\n",
      "+-------+------+---+--------+\n",
      "| naresh|     M| 30|  python|\n",
      "|   ravi|     M| 22|       c|\n",
      "|akansha|     F| 34|    java|\n",
      "| ravina|     F| 33|    ruby|\n",
      "|  bhanu|     M| 35|    java|\n",
      "|  vikas|     M| 11|   shell|\n",
      "|   jiya|     F| 39|    perl|\n",
      "+-------+------+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import HiveContext, sql\n",
    "\n",
    "hqlCtx = HiveContext(sc)\n",
    "\n",
    "print \"\\nDropping Hive Table if exists\"\n",
    "hqlCtx.sql(\"DROP TABLE IF EXISTS default.nartest\")\n",
    "\n",
    "print \"\\nSave Data Frame to Hive table default.nartest\"\n",
    "df.write.saveAsTable(\"default.nartest\")\n",
    "\n",
    "print \"\\nQuery Newly created Hive Table \"\n",
    "hqlCtx.sql(\"SELECT * FROM default.nartest\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using SparkSession (Spark Version >= 2.x)\n",
    "\n",
    "Whereas in Spark 2.0 the same effects can be achieved through SparkSession, without expliciting creating SparkConf, SparkContext, SQLContext or HiveContext as they’re encapsulated within the SparkSession. Using a builder design pattern, it instantiates a SparkSession object if one does not already exist, along with its associated underlying contexts.\n",
    "\n",
    "We will only Create SparkSession ‘spark’ and will do the same stuff (from previous code).\n",
    "\n",
    "pyspark.sql.SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating sconf variable using SparkConf, which will be used for SparkSesssion Creation - You can give the conf using SparkSessions 'config' as well. We are using SparkConf here just to show how we use SparkConf in SparkSession\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sconf = SparkConf().set(\"spark.executor.memory\",\"6g\").set(\"spark.executor.cores\",\"3\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating SparkSession 'myspark'\n",
    "\n",
    "SparkSession: New entry point for Spark functionality enabling programming Spark with the Dataset and DataFrame API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myspark = SparkSession.builder.master(\"yarn\").appName(\"SparkSession_Useage\")\\\n",
    "        .config(conf=sconf).enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Get Spark Executor Cores using spark.conf.get {same as sconf.get} : \n",
      "3\n",
      "\n",
      "Set Spark Executor Cores to 4 using spark.conf.set {same as sconf.set} \n",
      "\n",
      "Get Spark New Executor Cores using spark.conf.get {same as sconf.get} : \n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print \"\\nGet Spark Executor Cores using spark.conf.get {same as sconf.get} : \"\n",
    "print myspark.conf.get(\"spark.executor.cores\")\n",
    "\n",
    "print \"\\nSet Spark Executor Cores to 4 using spark.conf.set {same as sconf.set} \"\n",
    "myspark.conf.set(\"spark.executor.cores\",\"3\")\n",
    "\n",
    "print \"\\nGet Spark New Executor Cores using spark.conf.get {same as sconf.get} : \"\n",
    "print myspark.conf.get(\"spark.executor.cores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using spark.sparkContext.parallelize without creating any SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RDD to DF using rdd.toDF() and use df.show() \n",
      " \n",
      "+------+---+\n",
      "|    _1| _2|\n",
      "+------+---+\n",
      "|Naresh|  1|\n",
      "| Bhanu|  2|\n",
      "|  Ravi|  3|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd = myspark.sparkContext.parallelize([('Naresh',1),('Bhanu',2),('Ravi',3)])\n",
    "\n",
    "print \"\\nRDD to DF using rdd.toDF() and use df.show() \\n \"\n",
    "\n",
    "rdd.toDF().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## myspark: To Create DataFrames without SQLContext\n",
    "\n",
    "The data file used below is available in this repo under data folder. LOAD this file in the HDFS at /tmp directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LOAD and HDFS file using spark.sparkContext.textFile and Create DF using spark.createDataFrame() \n",
      "\n",
      "Printing DataFrame \n",
      "\n",
      "+-------+------+---+--------+\n",
      "|   name|gender|age|language|\n",
      "+-------+------+---+--------+\n",
      "| naresh|     M| 30|  python|\n",
      "|   ravi|     M| 22|       c|\n",
      "|akansha|     F| 34|    java|\n",
      "| ravina|     F| 33|    ruby|\n",
      "|  bhanu|     M| 35|    java|\n",
      "|  vikas|     M| 11|   shell|\n",
      "|   jiya|     F| 39|    perl|\n",
      "+-------+------+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print \"\\nLOAD and HDFS file using spark.sparkContext.textFile and Create DF using spark.createDataFrame() \"\n",
    "\n",
    "file = myspark.sparkContext.textFile(\"/tmp/people.txt\")\n",
    "\n",
    "line = file.map(lambda x: x.split(\" \")).map(lambda t: (t[0],t[1],int(t[2]),t[3]))\n",
    "\n",
    "df = myspark.createDataFrame(line,[\"name\",\"gender\",\"age\",\"language\"])\n",
    "\n",
    "print \"\\nPrinting DataFrame \\n\"\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## myspark: To access Hive DB/Tables or save data to Hive without HiveContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dropping Hive Table if exists using spark.sql \n",
      "\n",
      "Save Data Frame to Hive table default.nartest\n",
      "\n",
      "Query Newly created Hive Table\n",
      "\n",
      "+-------+------+---+--------+\n",
      "|   name|gender|age|language|\n",
      "+-------+------+---+--------+\n",
      "| naresh|     M| 30|  python|\n",
      "|   ravi|     M| 22|       c|\n",
      "|akansha|     F| 34|    java|\n",
      "| ravina|     F| 33|    ruby|\n",
      "|  bhanu|     M| 35|    java|\n",
      "|  vikas|     M| 11|   shell|\n",
      "|   jiya|     F| 39|    perl|\n",
      "+-------+------+---+--------+\n",
      "\n",
      "\n",
      "Deleting the table default.mytest\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print \"\\nDropping Hive Table if exists using spark.sql \"\n",
    "myspark.sql(\"DROP TABLE IF EXISTS default.mytest\")\n",
    "\n",
    "print \"\\nSave Data Frame to Hive table default.nartest\"\n",
    "df.write.saveAsTable(\"default.mytest\")\n",
    "\n",
    "print \"\\nQuery Newly created Hive Table\\n\"\n",
    "myspark.sql(\"SELECT * FROM default.mytest\").show()\n",
    "\n",
    "print \"\\nDeleting the table default.mytest\"\n",
    "myspark.sql(\"DROP TABLE default.mytest\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Setting up Run Time Properties\n",
    "    Only applicable while running spark job with spark2-submit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up new configuration for sparkContext/SparkSession.\n",
    "\n",
    "Any runtime property change will ONLY happen using SparkConf() or using SparkSession.config() or passing them as arguments.\n",
    "\n",
    "We did below earlier which is only a way/test to check the properties. If you open Spark UI and check these properties, you will find the default one there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Get Spark Executor Cores using spark.conf.get {same as sconf.get} : \n",
      "3\n",
      "\n",
      "Set Spark Executor Cores to 4 using spark.conf.set {same as sconf.set} \n",
      "\n",
      "Get Spark New Executor Cores using spark.conf.get {same as sconf.get} : \n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print \"\\nGet Spark Executor Cores using spark.conf.get {same as sconf.get} : \"\n",
    "print myspark.conf.get(\"spark.executor.cores\")\n",
    "\n",
    "print \"\\nSet Spark Executor Cores to 4 using spark.conf.set {same as sconf.set} \"\n",
    "myspark.conf.set(\"spark.executor.cores\",\"3\")\n",
    "\n",
    "print \"\\nGet Spark New Executor Cores using spark.conf.get {same as sconf.get} : \"\n",
    "print myspark.conf.get(\"spark.executor.cores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Using SparkConf()\n",
    "\n",
    "To set the properties permanently, create a new SparkConf() object, set the required properties there and use that to create a SparkContext/SparkSession. \n",
    "\n",
    "Use spark.sparkContext.getConf().getAll() to get all properties.\n",
    "\n",
    "You can also check the same in Spark UI under “Environment” Tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "conf = SparkConf().setAll([(\"spark.executor.cores\",\"2\"),(\"spark.executor.memory\",\"5g\"),(\"hive.exec.dynamic.partition\",\"true\")])\n",
    "myspark.stop()\n",
    "myspark = SparkSession.builder.master(\"yarn\").appName(\"Spark_Conf_Usage\").config(conf=conf).enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'5g'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myspark.sparkContext.getConf().get(\"spark.executor.memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To see all Properties\n",
    "#spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Using SparkSession.config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sparkwarehouseLocation = \"/tmp/spark-warehouse\"\n",
    "\n",
    "myspark.stop()\n",
    "myspark = SparkSession.builder.master(\"yarn\").appName(\"Spark_Conf_Usage\")\\\n",
    "        .config(\"spark.sql.warehouse.dir\",sparkwarehouseLocation)\\\n",
    "        .config(\"spark.serializer\",\"org.apache.spark.serializer.KryoSerializer\")\\\n",
    "        .config(\"spark.kryoserializer.buffer.max\",\"1g\")\\\n",
    "        .config(\"hive.exec.dynamic.partition\", \"true\")\\\n",
    "        .config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\")\\\n",
    "        .enableHiveSupport().getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'1g'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myspark.sparkContext.getConf().get(\"spark.kryoserializer.buffer.max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/tmp/spark-warehouse'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myspark.sparkContext.getConf().get(\"spark.sql.warehouse.dir\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's Next\n",
    "1. To Download this Single Notebook, go to Click this file in my Github Account, Copy the URL and paste in http://nbviewer.jupyter.org/. Download button will be in top right corner.\n",
    "\n",
    "2. Open your Juypter Notebook home page and upload using \"upload\" Button.\n",
    "\n",
    "3. Continue Learning from the next Notebook Spark_02_DataFrame_Operations.ipynb"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
