{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Creating UDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Data Frame with sample Data/schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myspark = SparkSession.builder.appName(\"UDFTest\").enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp = [[8.7,0.6,\"Delhi\"],[9.7,0.9,\"Gurgaon\"],[11.0,1.6,\"Noida\"],[11.2,-0.5,\"Jaipur\"],[9.0,-1.0,\"Hisar\"]]\n",
    "df = myspark.createDataFrame(temp,[\"avgHigh\",\"avgLow\",\"city\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- avgHigh: double (nullable = true)\n",
      " |-- avgLow: double (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------+\n",
      "|avgHigh|avgLow|   city|\n",
      "+-------+------+-------+\n",
      "|    8.7|   0.6|  Delhi|\n",
      "|    9.7|   0.9|Gurgaon|\n",
      "|   11.0|   1.6|  Noida|\n",
      "|   11.2|  -0.5| Jaipur|\n",
      "|    9.0|  -1.0|  Hisar|\n",
      "+-------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------+\n",
      "|avgHigh|avgLow|   city|\n",
      "+-------+------+-------+\n",
      "|    8.7|   0.6|  Delhi|\n",
      "|    9.7|   0.9|Gurgaon|\n",
      "|   11.0|   1.6|  Noida|\n",
      "|   11.2|  -0.5| Jaipur|\n",
      "|    9.0|  -1.0|  Hisar|\n",
      "+-------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"mytable\")\n",
    "myspark.sql(\"SELECT * FROM mytable\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. UDF for a Dataframe\n",
    "\n",
    "Creating and Registering UDF using psyaprk.sql.functions.udf\n",
    "\n",
    "This takes 2 arguments : Function Name and Return Type(Default is StringType)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Creating the Function to convert Celsius to Fahrenheit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def changeToF(degreesCelsius):\n",
    "    Far = (degreesCelsius * 9.0 / 5.0) + 32.0\n",
    "    return Far"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Registering newly created Function as SPARK UDF (using pyspark.sql.functions.udf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_udf = F.udf(changeToF, T.StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you can use lambda directly (instead of creating the Function separately)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_udf = F.udf(lambda degreesCelsius: (degreesCelsius * 9.0 / 5.0) + 32.0 , T.DoubleType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can use df_udf by pass any column to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------+-----------+\n",
      "|avgHigh|avgLow|   city|avgLow_in_F|\n",
      "+-------+------+-------+-----------+\n",
      "|    8.7|   0.6|  Delhi|      33.08|\n",
      "|    9.7|   0.9|Gurgaon|      33.62|\n",
      "|   11.0|   1.6|  Noida|      34.88|\n",
      "|   11.2|  -0.5| Jaipur|       31.1|\n",
      "|    9.0|  -1.0|  Hisar|       30.2|\n",
      "+-------+------+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"avgHigh\", \"avgLow\", \"city\", df_udf(\"avgLow\").alias(\"avgLow_in_F\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: This UDF can't be used on a TABLE\n",
    "\n",
    "    myspark.sql(\"SELECT myudf(avgLow) from mytable\")\n",
    "\n",
    "    ERROR: AnalysisException: u\"Undefined function: 'myudf'. This function is neither a registered temporary function nor a permanent function registered in the database 'default'.; line 1 pos 7\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. UDF for a Table/View\n",
    "\n",
    "Creating and Registering UDF using psyaprk.sql.SparkSession.udf (it is equivalent to sqlContext.udf.register)\n",
    "\n",
    "    Registers a python function (including lambda function) as a UDF so IT CAN BE USED IN SQL STATEMENT.\n",
    "\n",
    "This takes 3 Parameters:\n",
    "\n",
    "    (i) name – name of the UDF, (ii) f – python function , (iii) returnType – a pyspark.sql.types.DataType object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myspark.udf.register(\"table_udf\", lambda degreesCelsius: (degreesCelsius * 9.0 / 5.0) + 32 , T.DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------+-----------+\n",
      "|avgHigh|avgLow|city   |avgLow_in_F|\n",
      "+-------+------+-------+-----------+\n",
      "|8.7    |0.6   |Delhi  |33.08      |\n",
      "|9.7    |0.9   |Gurgaon|33.62      |\n",
      "|11.0   |1.6   |Noida  |34.88      |\n",
      "|11.2   |-0.5  |Jaipur |31.1       |\n",
      "|9.0    |-1.0  |Hisar  |30.2       |\n",
      "+-------+------+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myspark.sql(\"SELECT avgHigh, avgLow, city , table_udf(avgLow) as avgLow_in_F from mytable\").show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same thing we can achieve with sqlContext.registerFunction or sqlContext.udf.register.\n",
    "\n",
    "#### (i) Using \"sqlContext.udf.register\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext.udf.register(\"table_udf\", lambda degreesCelsius: (degreesCelsius * 9.0 / 5.0) + 32 , T.DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------+-----------+\n",
      "|avgHigh|avgLow|city   |avgLow_in_F|\n",
      "+-------+------+-------+-----------+\n",
      "|8.7    |0.6   |Delhi  |33.08      |\n",
      "|9.7    |0.9   |Gurgaon|33.62      |\n",
      "|11.0   |1.6   |Noida  |34.88      |\n",
      "|11.2   |-0.5  |Jaipur |31.1       |\n",
      "|9.0    |-1.0  |Hisar  |30.2       |\n",
      "+-------+------+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myspark.sql(\"SELECT avgHigh, avgLow, city , table_udf(avgLow) as avgLow_in_F from mytable\").show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (ii) Using \"sqlContext.registerFunction\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext.registerFunction(\"table_udf\", lambda degreesCelsius: (degreesCelsius * 9.0 / 5.0) + 32 , T.DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------+-----------+\n",
      "|avgHigh|avgLow|city   |avgLow_in_F|\n",
      "+-------+------+-------+-----------+\n",
      "|8.7    |0.6   |Delhi  |33.08      |\n",
      "|9.7    |0.9   |Gurgaon|33.62      |\n",
      "|11.0   |1.6   |Noida  |34.88      |\n",
      "|11.2   |-0.5  |Jaipur |31.1       |\n",
      "|9.0    |-1.0  |Hisar  |30.2       |\n",
      "+-------+------+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myspark.sql(\"SELECT * , table_udf(avgLow) AS avgLow_in_F from mytable\").show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Note: This table_udf can't be used on a DF\n",
    "\n",
    "    df.select(table_udf(df.avgLow)).show()\n",
    "\n",
    "    NameError: name 'table_udf' is not defined\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's Next\n",
    "\n",
    "1) To Download this Single Notebook, go to Click this file in my Github Account, Copy the URL and paste in http://nbviewer.jupyter.org/. Download button will be in top right corner.\n",
    "\n",
    "2) Open your Juypter Notebook home page and upload using \"upload\" Button.\n",
    "\n",
    "3) Continue Learning from the next Notebook Spark_06_ETL_Operations_Exercise.ipynb"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
