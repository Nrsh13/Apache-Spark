{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# ETL Operations using SPARK 2.x\n",
    "\n",
    "1) Change the timestamp formats to yyyy-MM-dd HH:mm:ss (remove the millisecond part)\n",
    "\n",
    "2) Generate a new column \"year_left\" showing years left in passport expiration\n",
    "\n",
    "3) Check the number of invalid mobile numbers (having 11 digits) and create a new column telling the status 'invalid\" or \"valid\"\n",
    "\n",
    "4) Create a new Column with Network status (BNZ or Others) based on IP address starting with 190-199\n",
    "\n",
    "5) Find out Invalid Email address, drop those records and get a new DF 'goodata'\n",
    "\n",
    "6) Find People having SAME fname, lname and email. Drop these Duplicate Records\n",
    "\n",
    "7) Create a new column 'priority' saying CRITICAL if passport expire left years are less than 5\n",
    "\n",
    "8) Creat a new Pricipal taking first 3 letters of fname, full lname and appending \"@HADOOP.COM\"\n",
    "\n",
    "9) Use of important pyspark.sql.functions.lit , replace, when, otherwise, substr, fill, isnan, isnull, ltrim, rtrim, udf, upper,lower\n",
    "\n",
    "10) Save the final data to Hive Dynamically Partitioned Table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data file used below was generated using https://github.com/Nrsh13/Random_Data_Generator/blob/master/random_data_generator.py. The generated data file is available under data Folder in this repo. LOAD this file in the HDFS at /tmp directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "myspark=SparkSession.builder.appName(\"Spark Operations\").master(\"yarn\").enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = spark.read.format(\"csv\").options(header=True, inferSchema=True, sep=\"\\t\",\n",
    "    dateFormat=\"yyyy-MM-dd\", timestampFormat=\"yyyy-MM-dd HH:mm:ss.ssssss\", ignoreLeadingWhiteSpace=True,\\\n",
    "    ignoreTrailingWhiteSpace=True, path=\"/tmp/etl_sampledata.tsv\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- fname: string (nullable = true)\n",
      " |-- lname: string (nullable = true)\n",
      " |-- principal: string (nullable = true)\n",
      " |-- emailid: string (nullable = true)\n",
      " |-- mobile: long (nullable = true)\n",
      " |-- passport_make: timestamp (nullable = true)\n",
      " |-- passport_expire: timestamp (nullable = true)\n",
      " |-- ipaddress: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Change the timestamp formats to yyyy-MM-dd HH:mm:ss (remove the millisecond part) \n",
    "\n",
    "    Functions Used: F.from_unixtime(), F.unix_timestamp(), F.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = data.withColumn(\"passport_make\", F.from_unixtime(F.unix_timestamp(data.passport_make, format='yyyy-MM-dd HH:mm:ss')))\\\n",
    "        .withColumn(\"passport_expire\", F.from_unixtime(F.unix_timestamp(data.passport_expire, format='yyyy-MM-dd HH:mm:ss')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generate a new column \"year_left\" showing years left in passport expiration\n",
    "\n",
    "    Functions Used: F.datediff(), F.current_timestamp(), F.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+----------+\n",
      "|passport_make      |passport_expire    |years_left|\n",
      "+-------------------+-------------------+----------+\n",
      "|2010-10-17 20:50:19|2020-10-17 20:50:19|2.63      |\n",
      "|2015-10-16 20:54:46|2025-10-16 20:54:46|7.63      |\n",
      "|2013-03-18 20:55:59|2023-03-18 20:55:59|5.05      |\n",
      "|2013-12-17 20:56:59|2023-12-17 20:56:59|5.8       |\n",
      "|2016-01-15 20:57:53|2026-01-15 20:57:53|7.88      |\n",
      "+-------------------+-------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = data.withColumn(\"years_left\", F.round(F.datediff(data.passport_expire,F.current_timestamp())/365,2))\n",
    "data.select(data.passport_make,data.passport_expire,data.years_left).show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Check the number of invalid mobile numbers (having 11 digits) and create a new column telling the status 'invalid\" or \"valid\"\n",
    "\n",
    "    Functions Used: F.lenght(), F.when().otherwise, F.substring(), F.groupBy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+\n",
      "|mobile     |mobile_status|\n",
      "+-----------+-------------+\n",
      "|99840995521|invalid      |\n",
      "|9863180471 |valid        |\n",
      "|9801241267 |valid        |\n",
      "|9898312853 |valid        |\n",
      "|9846430220 |valid        |\n",
      "+-----------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = data.withColumn(\"mobile_status\",F.when(F.length(data.mobile) <= 10, \"valid\").otherwise(\"invalid\"))\\\n",
    "        \n",
    "data.select(\"mobile\",\"mobile_status\").show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many invalid Mobile numbers were there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|mobile_status|count|\n",
      "+-------------+-----+\n",
      "|        valid| 8000|\n",
      "|      invalid| 2000|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.groupby(\"mobile_status\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.select(\"mobile\").filter(F.length(data.mobile) == 11).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the Mobile Number by removing 11th digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = data.withColumn(\"mobile\", F.substring(\"mobile\", 1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.select(\"mobile\").filter(F.length(data.mobile) == 11).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Create a new Column with Network status (BNZ or Others) based on IP address starting with 190-199\n",
    "\n",
    "    Functions Used: F.when().otherwise, F.substring_index(),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------+\n",
      "|      ipaddress|network|\n",
      "+---------------+-------+\n",
      "|  194.94.208.22|    BNZ|\n",
      "| 190.163.73.208|    BNZ|\n",
      "| 196.136.153.24|    BNZ|\n",
      "|191.106.144.130|    BNZ|\n",
      "|192.192.148.148|    BNZ|\n",
      "+---------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = data.withColumn(\"network\", \\\n",
    "            F.when(((F.substring_index(\"ipaddress\", \".\",1)) < 200) & ((F.substring_index(\"ipaddress\", \".\",1)) >= 190), \"BNZ\")\\\n",
    "            .otherwise(\"others\"))\n",
    "data.select(\"ipaddress\",\"network\").filter(data.network == \"BNZ\").show(5)\n",
    "#data.select(F.substring_index(\"ipaddress\", \".\",1)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- fname: string (nullable = true)\n",
      " |-- lname: string (nullable = true)\n",
      " |-- principal: string (nullable = true)\n",
      " |-- emailid: string (nullable = true)\n",
      " |-- mobile: string (nullable = true)\n",
      " |-- passport_make: string (nullable = true)\n",
      " |-- passport_expire: string (nullable = true)\n",
      " |-- ipaddress: string (nullable = true)\n",
      " |-- years_left: double (nullable = true)\n",
      " |-- mobile_status: string (nullable = false)\n",
      " |-- network: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Find out Invalid Email address, drop those records and get a new DF 'goodata'\n",
    "\n",
    "This includes emails which contains: @#, @@, ##, # and no '@'\n",
    "\n",
    "    Functions Used: F.like(), F.instr(), F.udf(), F.drop()\n",
    "\n",
    "What is the Number of Invalid Emails?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3127"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.select(\"emailid\").filter(data.emailid.like(\"%@@%\") \\\n",
    "                | data.emailid.like(\"%@#%\") | data.emailid.like(\"%##%\") \\\n",
    "                | data.emailid.like(\"%#%\") | (F.instr(data.emailid,\"@\") == 0 )).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding All invalid Emails using UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def validate_email(email):\n",
    "    mypat = r\"\\\"?([-a-zA-Z0-9.`?{}_]+@\\w+\\.\\w+)\\\"?\"   \n",
    "    pattern = re.compile(mypat)\n",
    "    if not re.match(pattern, email):\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "validate_email_udf = F.udf(validate_email, T.StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = data.withColumn(\"email_status\", validate_email_udf(data.emailid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3127"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.select(\"emailid\",\"email_status\").filter(data.email_status == \"false\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+------------+\n",
      "|emailid                   |email_status|\n",
      "+--------------------------+------------+\n",
      "|Andrea_Martinezhotmail.com|false       |\n",
      "|Nathan_gupta@#tcs.com     |false       |\n",
      "|Robert_oorehotmail.com    |false       |\n",
      "|naresh_Rogers#gmail.com   |false       |\n",
      "|akash_Lewis@@yahoo.com    |false       |\n",
      "+--------------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(\"emailid\",\"email_status\").filter(data.email_status == \"false\").show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the Data having valid Emails Addresses. Drop the temporary columns mobile_status and email_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gooddata = data.filter(data.email_status == \"true\").drop(\"mobile_status\").drop(\"email_status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6873"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gooddata.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- fname: string (nullable = true)\n",
      " |-- lname: string (nullable = true)\n",
      " |-- principal: string (nullable = true)\n",
      " |-- emailid: string (nullable = true)\n",
      " |-- mobile: string (nullable = true)\n",
      " |-- passport_make: string (nullable = true)\n",
      " |-- passport_expire: string (nullable = true)\n",
      " |-- ipaddress: string (nullable = true)\n",
      " |-- years_left: double (nullable = true)\n",
      " |-- network: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gooddata.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Find People having SAME fname, lname and email. Drop these Duplicate Records\n",
    "\n",
    "Will use fname, lname and emailid to decide if a record is duplicate or NOT.\n",
    "\n",
    "    Functions Used: F.dropDuplicates(), F.collect_list(), F.size(), F.agg(), df.distinct()\n",
    "\n",
    "Counting Distinct Records out of Total Good Records - 6873"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6708"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#gooddata.groupBy(\"fname\",\"lname\",\"emailid\").count().sort(\"count\").show()\n",
    "gooddata.createOrReplaceTempView(\"mytable\")\n",
    "spark.sql(\"SELECT DISTINCT (fname, lname, emailid) FROM mytable\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping Duplicates recrods using dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gooddata = gooddata.dropDuplicates([\"fname\",\"lname\", \"emailid\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if there are still any Duplicate records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gooddata.groupBy(\"fname\",\"lname\",\"emailid\").agg(F.collect_list(\"emailid\").alias(\"count\"))\\\n",
    "        .where(F.size(\"count\") > 1).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we left with 6708 Unique Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6708"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gooddata.select(\"fname\", \"lname\",\"emailid\").distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Create a new column 'priority' saying CRITICAL if passport expire left years are less than 5\n",
    "\n",
    "    Functions Used: F.when(), F.lit(), df.fillna() {alias for df.na.fill()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|years_left|priority|\n",
      "+----------+--------+\n",
      "|8.3       |LOW     |\n",
      "|0.55      |CRITICAL|\n",
      "|5.97      |LOW     |\n",
      "|2.38      |CRITICAL|\n",
      "|2.97      |CRITICAL|\n",
      "+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gooddata = gooddata.withColumn(\"priority\",F.when(gooddata.years_left < 3 , F.lit(\"CRITICAL\"))).fillna(\"LOW\")\n",
    "gooddata.select(\"years_left\",\"priority\").show(5,False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Creating a new Pricipal taking first 3 letters of fname, full lname and appending \"@HADOOP.COM\"\n",
    "\n",
    "    Functions Used: F.concat(), F.substring(), F.lower(), F.lit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+----------------------+\n",
      "|fname  |lname   |principal             |\n",
      "+-------+--------+----------------------+\n",
      "|Frank  |Campbell|fracampbell@HADOOP.COM|\n",
      "|Henry  |Howard  |henhoward@HADOOP.COM  |\n",
      "|tom    |rown    |tomrown@HADOOP.COM    |\n",
      "|stephen|oore    |steoore@HADOOP.COM    |\n",
      "|Richard|Cooper  |riccooper@HADOOP.COM  |\n",
      "+-------+--------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gooddata = gooddata.withColumn(\"principal\", F.concat(F.lower(F.substring(gooddata[\"fname\"],1,3))\\\n",
    "        , F.lower(gooddata[\"lname\"]), F.lit(\"@HADOOP.COM\")))\n",
    "gooddata.select(\"fname\",\"lname\",\"principal\").show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 9. Use of pyspark.sql.functions like lit , replace, when, otherwise, substr, fill, isnan, isnull, ltrim, rtrim, udf, upper,lower, Window.\n",
    "\n",
    "    All used in previouse Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 10. Save the final data to Hive Dynamically Partitioned Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### i. Create the Table in Advance and save the data directly to HDFS location using df.write.save()\n",
    "\n",
    "Creating a Dynamic partition Table default.etl_operations_result partitioned on \"network\" and \"priority\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gooddata.createOrReplaceTempView(\"mytable\")\n",
    "\n",
    "myspark.conf.set(\"hive.exec.dynamic.partition\", \"true\")\n",
    "myspark.conf.set(\"hive.exec.dynamic.partition.mode\", \"nonstrict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myspark.sql(\"DROP TABLE IF EXISTS default.etl_operations_result\")\n",
    "\n",
    "myspark.sql(\"CREATE TABLE IF NOT EXISTS default.etl_operations_result( \\\n",
    "            fname string, lname string,`principal` string,`emailid` string, \\\n",
    "             `mobile` string, `passport_make` string, `passport_expire` string, `ipaddress` string, `years_left` double) \\\n",
    "            PARTITIONED BY (network string, priority string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' \\\n",
    "            LOCATION '/tmp/etl_operations_result'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing the Data to \"/tmp/etl_operations_result\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myspark.conf.set(\"spark.sql.shuffle.partitions\",5)\n",
    "\n",
    "gooddata.write.save(path = \"/tmp/etl_operations_result\", format=\"csv\", mode=\"overwrite\", partitionBy=(\"network\",\"priority\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|    6708|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myspark.sql(\"MSCK REPAIR TABLE default.etl_operations_result\")\n",
    "myspark.sql(\"SELECT COUNT(*) FROM default.etl_operations_result\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OR\n",
    "\n",
    "ii. Create the Table in Advance and load using INSERT OVERWRITE COMMAND. Make sure the order of columns is correct and partitioned Column 'network' and 'priority' is used in the end of SELECT command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gooddata.createOrReplaceTempView(\"mytable\")\n",
    "\n",
    "myspark.conf.set(\"hive.exec.dynamic.partition\", \"true\")\n",
    "myspark.conf.set(\"hive.exec.dynamic.partition.mode\", \"nonstrict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myspark.sql(\"INSERT OVERWRITE TABLE default.etl_operations_result PARTITION(network, priority)\\\n",
    "            SELECT fname , lname ,principal ,emailid, mobile, passport_make, passport_expire,\\\n",
    "            ipaddress, years_left, network, priority \\\n",
    "            FROM mytable\\\n",
    "            \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|    6708|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myspark.sql(\"SELECT COUNT(*) FROM default.etl_operations_result\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's Next\n",
    "\n",
    "1) To Download this Single Notebook, go to Click this file in my Github Account, Copy the URL and paste in http://nbviewer.jupyter.org/. Download button will be in top right corner.\n",
    "\n",
    "2) Open your Juypter Notebook home page and upload using \"upload\" Button."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
