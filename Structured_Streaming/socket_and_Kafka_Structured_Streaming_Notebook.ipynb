{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-requisites\n",
    "* Should have already running Kafka Cluster. The Topic 'mytopic' will be created automatically once we start the Read Stream.\n",
    "* This code assumes all services like Kafka/Socket Server/Zookeeper are running on the same machine where Jupyter Notebook Process was triggered. If not, update the hostname in Step 3.\n",
    "* Make sure you have included required streaming jars while starting Jupyter.\n",
    "\n",
    "Example:\n",
    "\n",
    "```PYSPARK_DRIVER_PYTHON_OPTS=\"ANACONDA_HOME/bin/jupyter-notebook --ip hostname --no-browser --notebook-dir HOME/jupyter_notebook_spark\" pyspark --jars HOME/jars/spark-sql-kafka-0-10_2.11-2.3.1.jar,KAFKA_HOME/libs/kafka-clients-2.0.0.jar --master yarn-client --executor-memory 8g --num-executors 3 --executor-cores 4 --name Notebook &```\n",
    "\n",
    "Jars Download Link:\n",
    "    1. wget http://central.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.11/2.3.1/spark-sql-kafka-0-10_2.11-2.3.1.jar\n",
    "    2. kafka-clients-2.0.0.jar can be found at $KAFKA_HOME/libs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: \n",
    "* Importing Required Python/SparkSQL Modules\n",
    "* Stop any Active Query.\n",
    "\n",
    "Different Managing Actions on the query-\n",
    "```\n",
    "query.id // get the unique identifier of the running query that persists across restarts from checkpoint data\n",
    "query.runId // get the unique id of this run of the query, which will be generated at every start/restart\n",
    "query.name  // get the name of the auto-generated or user-specified name\n",
    "query.explain()  // print detailed explanations of the query\n",
    "query.stop()  // stop the query\n",
    "query.awaitTermination()  // block until query is terminated, with stop() or with error\n",
    "query.exception  // the exception if the query has been terminated with error\n",
    "query.recentProgress  // an array of the most recent progress updates for this query\n",
    "query.lastProgress  // the most recent progress update of this streaming query```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import SparkSQL Modules\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "# Other Python Modules\n",
    "import os, sys, subprocess, json\n",
    "import datetime, time\n",
    "import socket, signal\n",
    "\n",
    "try:\n",
    "    if append_mode_query.status['message'] != 'Stopped':\n",
    "        append_mode_query.stop()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2:\n",
    "* Update the 'source' variable as required. \n",
    "* Available Options are 'socket' or 'kafka'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Input Source (socket or Kafka)\n",
    "source = 'KAFKA'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3:\n",
    "## Setting required variables:\n",
    "\n",
    "   * ### hostname: \n",
    "     * Server where netcat or kafka Broker is Running. If your source is running on a separate Machine, hardcode the 'hostname' variable to required value.\n",
    "    \n",
    "   * ### schema: \n",
    "      * To be applied on the Incoming Stream.\n",
    "      \n",
    "    By default, Structured Streaming from file based sources requires you to specify the schema, rather than rely on Spark to infer it automatically. This restriction ensures a consistent schema will be used for the streaming query, even in the case of failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Socket Server Details\n",
    "\n",
    "hostname = socket.gethostname()\n",
    "\n",
    "socketServer = hostname\n",
    "socketPort = 9999\n",
    "\n",
    "# Kafka Server Details\n",
    "kafkaBrokerServer = hostname\n",
    "kafkaBrokerPort = 9092\n",
    "zookeeperServer = hostname\n",
    "zookeeperPort = 2185\n",
    "\n",
    "kafkaBroker = kafkaBrokerServer+\":\"+str(kafkaBrokerPort)\n",
    "\n",
    "schema = T.StructType([T.StructField(\"fname\", T.StringType(), True),\\\n",
    "                         T.StructField(\"lname\", T.StringType(), True),\\\n",
    "                         T.StructField(\"email\", T.StringType(), True),\\\n",
    "                         T.StructField(\"principal\", T.StringType(), True),\\\n",
    "                         T.StructField(\"passport_make_date\", T.StringType(), True),\\\n",
    "                         T.StructField(\"passport_expiry_date\", T.StringType(), True),\\\n",
    "                         T.StructField(\"ipaddress\", T.StringType(), True),\\\n",
    "                         T.StructField(\"mobile\", T.StringType(), True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4:\n",
    "* Creating a function to validate an Email Address received in the Stream.\n",
    "* Registering this function as a spark UDF.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def validate_email(email_address):\n",
    "        try:\n",
    "                import re\n",
    "                match = re.match('^[_a-z0-9-]+(\\.[_a-z0-9-]+)*@[a-z0-9-]+(\\.[a-z0-9-]+)*(\\.[a-z]{2,4})$', email_address.lower())\n",
    "                if match == None:\n",
    "                        status = False\n",
    "                else:\n",
    "                        status = True\n",
    "                return status\n",
    "        except Exception,e:\n",
    "                print (\"Failed to Validate the Email Address !!\")\n",
    "                print (\"ERROR: \" , e)\n",
    "                \n",
    "# Register the Function as UDF\n",
    "validate_email_address = F.udf(validate_email, T.BooleanType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5:\n",
    "### Collecting the incoming Stream.\n",
    "     \n",
    "* .readStream: To start reading the Stream.\n",
    "* .format: The Data Source. Available values are Socket, Directory and Kafka.\n",
    "* .option: Takes the required details like socketServer or kafkaBroker. An important one '.option(\"sep\", \";\")' is to use desired filed separator.\n",
    "   \n",
    "### Printing the Default Schema of Raw Stream.\n",
    "    \n",
    "* The Streamed DataFrame contains one column of strings named “value” for sure, and each line in the streaming text data becomes a row in the table. \n",
    "* There can be other columns too depending on the Data Source. For eq. Kafka will have Columns - key, value, topic, partition, offset, timestamp and timestampType\n",
    "    \n",
    "### Creating recordDF DataFrame by applying required Schema on the Stream.\n",
    "    \n",
    "* .schema: Apply required schema on Column \"value\". This will give us all desired columns Drived from the streamed data.\n",
    "    \n",
    "### Printing the New Schema.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Printing Raw Kafka Stream Schema.\n",
      "\n",
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n",
      "\n",
      "Printing recordDF DataFrame Schema.\n",
      "\n",
      "root\n",
      " |-- fname: string (nullable = true)\n",
      " |-- lname: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- principal: string (nullable = true)\n",
      " |-- passport_make_date: string (nullable = true)\n",
      " |-- passport_expiry_date: string (nullable = true)\n",
      " |-- ipaddress: string (nullable = true)\n",
      " |-- mobile: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if source.lower() == 'socket':\n",
    "    socketStream = spark\\\n",
    "               .readStream\\\n",
    "               .format('socket')\\\n",
    "               .option('host', socketServer)\\\n",
    "               .option('port', socketPort)\\\n",
    "               .load()\n",
    "    \n",
    "    print \"\\nPrinting Raw Socket Stream Schema.\\n\"\n",
    "    \n",
    "    socketStream.printSchema()\n",
    "    \n",
    "    recordDF = socketStream.select(socketStream.value).where(socketStream.value != '')\n",
    "    \n",
    "    recordDF = recordDF.withColumn(\"value\", F.from_json(\"value\", schema)).select(F.col('value.*'))\n",
    "    \n",
    "    print \"\\nPrinting recordDF DataFrame Schema.\\n\"\n",
    "    \n",
    "    recordDF.printSchema()\n",
    "\n",
    "elif source.lower() == 'kafka':\n",
    "    kafkaStream = spark\\\n",
    "                .readStream\\\n",
    "                .format('kafka')\\\n",
    "                .option('kafka.bootstrap.servers', kafkaBroker)\\\n",
    "                .option('subscribe', 'mytopic')\\\n",
    "                .load()                \n",
    "    \n",
    "    print \"\\nPrinting Raw Kafka Stream Schema.\\n\"\n",
    "    \n",
    "    kafkaStream.printSchema()\n",
    "                \n",
    "    kafkaStream = kafkaStream.select(F.from_json(F.col(\"value\").cast(\"string\"), schema).alias(\"value\"))\n",
    "    \n",
    "    recordDF = kafkaStream.select(\"value.*\").where(kafkaStream.value.isNotNull())\n",
    "    \n",
    "    print \"\\nPrinting recordDF DataFrame Schema.\\n\"\n",
    "    \n",
    "    recordDF.printSchema()\n",
    "    \n",
    "else:\n",
    "    print \"Please choose sources - Socket or Kafka !!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6:\n",
    "## Applying basic Transformations like\n",
    "* Generating a new Column 'event_time'.\n",
    "* Renaming an existing Column.\n",
    "* Validating the email Column for valid Emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Adding the EVENT Time\n",
    "recordDF = recordDF.withColumn(\"event_time\", F.current_timestamp().cast(T.TimestampType()))\n",
    "\n",
    "# Just Renaming a Column\n",
    "recordDF = recordDF.select(\n",
    "              recordDF.fname,\n",
    "              recordDF.lname,\n",
    "              recordDF.email,\n",
    "              recordDF.passport_make_date,\n",
    "              recordDF.passport_expiry_date,\n",
    "              recordDF.ipaddress,\n",
    "              recordDF.mobile,\n",
    "              )\\\n",
    "              .withColumnRenamed(\n",
    "                \"principal\",\n",
    "                \"realm\"\n",
    "              )\n",
    "\n",
    "# Add new column having Email validity Status\n",
    "recordDF = recordDF.select(\"*\" , validate_email_address(recordDF.email).alias(\"is_email_valid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7:\n",
    "## Applying some more Transformations like\n",
    "* Change the Data Type of passport_make_year and passport_expiry_year from String to TimestampType().\n",
    "* Generate a new column showing the year for passport make and expire.\n",
    "* Choosing Required Columns.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Change the Data Type of passport_make_year and passport_expiry_year from String to TimestampType()\n",
    "recordDF = recordDF.withColumn(\"passport_make_date\", recordDF.passport_make_date.cast(T.TimestampType()))\\\n",
    "        .withColumn(\"passport_expiry_date\", recordDF.passport_expiry_date.cast(T.TimestampType()))\\\n",
    "        .withColumn(\"mobile\", recordDF.mobile.cast(T.LongType()))\n",
    "\n",
    "# Generate a new column showing the year for passport make and expire\n",
    "recordDF = recordDF.withColumn(\"passport_make_year\", F.year(recordDF.passport_make_date).cast(T.IntegerType()))\\\n",
    "        .withColumn(\"passport_expiry_year\", F.year(recordDF.passport_expiry_date).cast(T.IntegerType()))\n",
    "\n",
    "\n",
    "# Choose Required columns:\n",
    "recordDF = recordDF.select(\n",
    "              recordDF.fname,\n",
    "              recordDF.lname,\n",
    "              recordDF.email,\n",
    "              recordDF.mobile,\n",
    "              recordDF.passport_expiry_year,\n",
    "              recordDF.is_email_valid,\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8:\n",
    "### Writing the Transformed Streamed Data into Memory.\n",
    "* .queryName: Optionally, specify a unique name of the query for identification. Compulsory for format('memory').\n",
    "\n",
    "\n",
    "* .outputMode: \n",
    "    Append Mode: This is the default mode, where only the new rows added to the Result Table since the last trigger will be outputted to the sink.\n",
    "    Complete Mode: The whole Result Table will be outputted to the sink after every trigger. This is supported for aggregation queries\n",
    "    Update Mode: Only the rows in the Result Table that were updated since the last trigger will be outputted to the sink\n",
    "\n",
    "\n",
    "* .format: \n",
    "    Sink where to store the data. Available options - Memory, Console, File, Kafka, Foreach and ForeachBatch.\n",
    "    For File Sink, this option can take the output format like .format('parquet').\n",
    "\n",
    "\n",
    "* .option: Important options\n",
    "    ```\n",
    "    truncate: Whether to truncate the Columns having large values.\n",
    "    numRows: How many rows should be displayed.\n",
    "    checkpointLocation: Required for File and Kafka sinks.\n",
    "    path: Used for File sink. Path to an HDFS Location.\n",
    "    kafka.bootstrap.servers: Comma separated KafkaBroker:Port Details\n",
    "    topic: To which topic will be sending the data.\n",
    "    ```\n",
    "\n",
    "* .trigger: The trigger settings of a streaming query defines the timing of streaming data processing, whether the query is going to executed as micro-batch query with a fixed batch interval or as a continuous processing query.\n",
    "    ```\n",
    "    unspecified (default): If no trigger setting is explicitly specified, then by default, the query will be executed in micro-batch mode, where micro-batches will be generated as soon as the previous micro-batch has completed processing.\n",
    "\n",
    "    Fixed interval micro-batches: The query will be executed with micro-batches mode, where micro-batches will be kicked off at the user-specified intervals. If no new data is available, then no micro-batch will be kicked off.\n",
    "\n",
    "    One-time micro-batch\tThe query will execute *only one* micro-batch to process all the available data and then stop on its own. This is useful in scenarios you want to periodically spin up a cluster, process everything that is available since the last period, and then shutdown the cluster.\n",
    "\n",
    "    Continuous with fixed checkpoint interval: Experimental as of Spark verison 2.4.0\n",
    "    ```\n",
    "    \n",
    "* .start: Start the Streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "append_mode_query = recordDF.writeStream\\\n",
    "                      .queryName(\"append_mode_query\")\\\n",
    "                      .outputMode(\"append\")\\\n",
    "                      .format(\"memory\")\\\n",
    "                      .option(\"truncate\", \"false\")\\\n",
    "                      .option(\"numRows\", 5)\\\n",
    "                      .trigger(processingTime='5 seconds')\\\n",
    "                      .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9:\n",
    "## Sample Data for Input\n",
    "* If your source is a Socket Server, use below Command to Start Socket Server\n",
    "    ```\n",
    "    nc -lk hostname 9999\n",
    "    ```\n",
    "  \n",
    "* If your source is Kafka, use below Command to Start the Producer\n",
    "\n",
    "    ```\n",
    "    KAFKA_HOME/bin/kafka-console-producer.sh --broker-list kafkaBroker --topic mytopic\n",
    "    ```\n",
    "    \n",
    "* Use below sample Data as Input one by one. Make Sure you use the same Format.\n",
    "\n",
    "\n",
    "* ROUND 1: To Add some records\n",
    "\n",
    "{\"fname\" : \"Billy\",\"lname\" : \"Clark\",\"email\" : \"Billy_Clark@@yahoo.com\",\"principal\" : \"Billy@EXAMPLE.COM\",\"passport_make_date\" : \"2013-07-30 14:39:59.964057\",\"passport_expiry_date\" : \"2023-07-30 14:39:59.964057\",\"ipaddress\" : \"142.195.1.154\" , \"mobile\" : \"9819415434\"}\n",
    "\n",
    "{\"fname\" : \"Wayne\",\"lname\" : \"iller\",\"email\" : \"Wayne_iller@bnz.co.nz\",\"principal\" : \"Wayne@EXAMPLE.COM\",\"passport_make_date\" : \"2011-03-30 14:40:00.973376\",\"passport_expiry_date\" : \"2021-03-30 14:40:00.973376\",\"ipaddress\" : \"81.187.181.223\" , \"mobile\" : \"9828826164\"}\n",
    "\n",
    "{\"fname\" : \"Christian\",\"lname\" : \"weign\",\"email\" : \"Christian_weign@tcs.com\",\"principal\" : \"Christian@EXAMPLE.COM\",\"passport_make_date\" : \"2013-02-28 14:40:01.982722\",\"passport_expiry_date\" : \"2023-02-28 14:40:01.982722\",\"ipaddress\" : \"158.169.175.39\" , \"mobile\" : \"9870023860\"}\n",
    "\n",
    "{\"fname\" : \"Daniel\",\"lname\" : \"weign\",\"email\" : \"Daniel_weign#gmail.com\",\"principal\" : \"Daniel@EXAMPLE.COM\",\"passport_make_date\" : \"2016-07-29 14:40:02.992117\",\"passport_expiry_date\" : \"2026-07-29 14:40:02.992117\",\"ipaddress\" : \"203.81.64.23\" , \"mobile\" : \"9804581156\"}\n",
    "\n",
    "* ROUND 2: To Update the lname\n",
    "\n",
    "{\"fname\" : \"Thomas\",\"lname\" : \"dagarin\",\"email\" : \"Thomas_dagarin@@gmail.com\",\"principal\" : \"Thomas@EXAMPLE.COM\",\"passport_make_date\" : \"2013-10-29 14:40:05.003005\",\"passport_expiry_date\" : \"2023-10-29 14:40:05.003005\",\"ipaddress\" : \"108.125.242.158\" , \"mobile\" : \"9823308381\"}\n",
    "\n",
    "{\"fname\" : \"Russell\",\"lname\" : \"Wright\",\"email\" : \"Russell_Wright@nbc.com\",\"principal\" : \"Russell@EXAMPLE.COM\",\"passport_make_date\" : \"2012-07-30 14:40:06.011988\",\"passport_expiry_date\" : \"2022-07-30 14:40:06.011988\",\"ipaddress\" : \"64.169.155.253\" , \"mobile\" : \"9848316824\"}\n",
    "\n",
    "{\"fname\" : \"Kyle\",\"lname\" : \"kumar\",\"email\" : \"Kyle_kumar@hotmail.com\",\"principal\" : \"Kyle@EXAMPLE.COM\",\"passport_make_date\" : \"2012-04-30 14:40:07.021665\",\"passport_expiry_date\" : \"2022-04-30 14:40:07.021665\",\"ipaddress\" : \"198.196.75.63\" , \"mobile\" : \"9837699819\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 10:\n",
    "\n",
    "* Make sure Step 9 is ready to take your Input Data.\n",
    "* Query the Streamed Data like a Table.\n",
    "\n",
    "Filtering the Stream based on Valid email addresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+----------------------+----------+--------------------+--------------+\n",
      "|fname  |lname |email                 |mobile    |passport_expiry_year|is_email_valid|\n",
      "+-------+------+----------------------+----------+--------------------+--------------+\n",
      "|Russell|Wright|Russell_Wright@nbc.com|9848316824|2022                |true          |\n",
      "|Kyle   |kumar |Kyle_kumar@hotmail.com|9837699819|2022                |true          |\n",
      "+-------+------+----------------------+----------+--------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM append_mode_query where is_email_valid = 'true'\").show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
